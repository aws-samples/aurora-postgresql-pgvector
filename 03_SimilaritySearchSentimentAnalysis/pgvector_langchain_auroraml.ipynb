{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search with pgvector and Amazon Aurora PostgreSQL\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Use HuggingFace's sentence transformer model [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) to generate embeddings\n",
    "2. Store and query vector embeddings using pgvector in Aurora PostgreSQL  \n",
    "3. Implement semantic search using LangChain's vector store capabilities\n",
    "4. Calculate similarity scores between text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install required Python libraries for the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentencepiece installed\n"
     ]
    }
   ],
   "source": [
    "# Install sentencepiece for tokenization (required by transformer models)\n",
    "!conda install -c conda-forge sentencepiece -y > /dev/null 2>&1\n",
    "print(\"‚úÖ Sentencepiece installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "# Core dependencies\n",
    "langchain==0.2.16\n",
    "langchain-community==0.2.17\n",
    "langchain-postgres==0.0.15\n",
    "psycopg2-binary==2.9.10\n",
    "pgvector==0.2.5\n",
    "python-dotenv==1.0.0\n",
    "sentence-transformers>=2.5.0\n",
    "huggingface-hub>=0.20.0\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "torch\n",
    "transformers>=4.36.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install all packages\n",
    "!pip install -r requirements.txt -q\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment and Import Libraries\n",
    "\n",
    "Import required libraries and initialize the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8989/3693577970.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment setup complete!\n",
      "üìä Using embedding model: all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries and setup environment\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Import core libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üìä Using embedding model: all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Database Connection\n",
    "\n",
    "Set up connection parameters for Aurora PostgreSQL with pgvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Database Configuration:\n",
      "   Host: apgpg-pgvector.cluster-c7kkeakuk3cl.us-west-2.rds.amazonaws.com\n",
      "   Database: postgres\n",
      "   Collection: hotel_reviews_langchain\n"
     ]
    }
   ],
   "source": [
    "# Database connection configuration\n",
    "import os\n",
    "\n",
    "DB_HOST = os.getenv('PGVECTOR_HOST', 'localhost')\n",
    "DB_PORT = os.getenv('PGVECTOR_PORT', '5432')\n",
    "DB_NAME = os.getenv('PGVECTOR_DATABASE', 'postgres')\n",
    "DB_USER = os.getenv('PGVECTOR_USER', 'postgres')\n",
    "DB_PASSWORD = os.getenv('PGVECTOR_PASSWORD', 'password')\n",
    "DB_DRIVER = 'psycopg2'\n",
    "\n",
    "# Build connection string\n",
    "CONNECTION_STRING = f\"postgresql+{DB_DRIVER}://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Collection name for vector store\n",
    "COLLECTION_NAME = \"hotel_reviews_langchain\"\n",
    "\n",
    "# Display configuration (masking password)\n",
    "display_connection = CONNECTION_STRING.replace(DB_PASSWORD, \"****\")\n",
    "print(f\"üìä Database Configuration:\")\n",
    "print(f\"   Host: {DB_HOST}\")\n",
    "print(f\"   Database: {DB_NAME}\")\n",
    "print(f\"   Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "Load hotel reviews data from CSV file or create sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Data file not found. Creating sample data...\n",
      "‚úÖ Created sample data file with 100 reviews\n",
      "‚úÖ Loaded 100 documents\n",
      "\n",
      "Sample reviews:\n",
      "1. comments: Excellent service and beautiful rooms. The staff was very helpful and the breakfast was am...\n",
      "2. comments: Great location near the beach. Pool area was fantastic! Very family friendly....\n",
      "3. comments: Amazing mountain views. Perfect for a peaceful getaway. Very quiet and relaxing....\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check for data file\n",
    "data_file = './data/fictitious_hotel_reviews_trimmed_500.csv'\n",
    "if not os.path.exists(data_file):\n",
    "    print(\"‚ö†Ô∏è Data file not found. Creating sample data...\")\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    \n",
    "    # Create sample hotel reviews\n",
    "    sample_reviews = [\n",
    "        \"Excellent service and beautiful rooms. The staff was very helpful and the breakfast was amazing.\",\n",
    "        \"Great location near the beach. Pool area was fantastic! Very family friendly.\",\n",
    "        \"Amazing mountain views. Perfect for a peaceful getaway. Very quiet and relaxing.\",\n",
    "        \"Convenient location but rooms were a bit small. Good value for money though.\",\n",
    "        \"Beautiful lake views. Restaurant food was delicious. Will definitely come back.\",\n",
    "        \"The room was spotlessly clean and the bed was very comfortable. Great night's sleep.\",\n",
    "        \"Staff went above and beyond to help us. Really appreciated their hospitality.\",\n",
    "        \"Loved the spa facilities. Very relaxing atmosphere throughout the hotel.\",\n",
    "        \"Business center was well equipped. Perfect for work trips.\",\n",
    "        \"Kids loved the pool and game room. Great family vacation spot.\",\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    sample_data = pd.DataFrame({'comments': sample_reviews * 10})  # Duplicate for more data\n",
    "    data_file = './data/hotel_reviews.csv'\n",
    "    sample_data.to_csv(data_file, index=False)\n",
    "    print(f\"‚úÖ Created sample data file with {len(sample_data)} reviews\")\n",
    "\n",
    "# Load data using LangChain's CSVLoader\n",
    "loader = CSVLoader(\n",
    "    file_path=data_file,\n",
    "    encoding='utf-8',\n",
    "    csv_args={'delimiter': ','}\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(data)} documents\")\n",
    "print(f\"\\nSample reviews:\")\n",
    "for i, doc in enumerate(data[:3], 1):\n",
    "    print(f\"{i}. {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split 100 documents into 100 chunks\n",
      "Average chunk size: 86 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"‚úÖ Split {len(data)} documents into {len(docs)} chunks\")\n",
    "print(f\"Average chunk size: {sum(len(d.page_content) for d in docs) / len(docs):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store and Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating vector store collection...\n",
      "‚è≥ This may take a minute...\n",
      "‚úÖ Vector store created successfully!\n",
      "üìä Collection: hotel_reviews_langchain\n",
      "üìù Documents indexed: 100\n"
     ]
    }
   ],
   "source": [
    "# Create PGVector instance and store documents\n",
    "print(\"üöÄ Creating vector store collection...\")\n",
    "print(\"‚è≥ This may take a minute...\")\n",
    "\n",
    "try:\n",
    "    db = PGVector.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        connection=CONNECTION_STRING,\n",
    "        pre_delete_collection=True  # Clean start\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Vector store created successfully!\")\n",
    "    print(f\"üìä Collection: {COLLECTION_NAME}\")\n",
    "    print(f\"üìù Documents indexed: {len(docs)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating vector store: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check database connection settings\")\n",
    "    print(\"2. Ensure pgvector extension is installed: CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    print(\"3. Verify database user has necessary permissions\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'What do some of the positive reviews say?'\n",
      "üìä Found 5 matches\n",
      "============================================================\n",
      "\n",
      "Result 1:\n",
      "üìà Similarity Score: 0.5726\n",
      "üìÑ Content: comments: Convenient location but rooms were a bit small. Good value for money though....\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "üìà Similarity Score: 0.5726\n",
      "üìÑ Content: comments: Convenient location but rooms were a bit small. Good value for money though....\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 3:\n",
      "üìà Similarity Score: 0.5726\n",
      "üìÑ Content: comments: Convenient location but rooms were a bit small. Good value for money though....\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 4:\n",
      "üìà Similarity Score: 0.5726\n",
      "üìÑ Content: comments: Convenient location but rooms were a bit small. Good value for money though....\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 5:\n",
      "üìà Similarity Score: 0.5726\n",
      "üìÑ Content: comments: Convenient location but rooms were a bit small. Good value for money though....\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define search query\n",
    "query = \"What do some of the positive reviews say?\"\n",
    "\n",
    "# Perform similarity search with scores\n",
    "docs_with_score = db.similarity_search_with_score(query, k=5)\n",
    "\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(f\"üìä Found {len(docs_with_score)} matches\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display results\n",
    "for i, (doc, score) in enumerate(docs_with_score, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"üìà Similarity Score: {score:.4f}\")\n",
    "    print(f\"üìÑ Content: {doc.page_content[:200]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Retriever for Chain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created retriever with cosine similarity\n",
      "\n",
      "üîç Test Query: 'excellent service'\n",
      "üìä Retrieved 4 documents\n",
      "\n",
      "Document 1: comments: Staff went above and beyond to help us. Really appreciated their hospitality....\n",
      "\n",
      "Document 2: comments: Staff went above and beyond to help us. Really appreciated their hospitality....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_postgres.vectorstores import DistanceStrategy\n",
    "\n",
    "# Create vector store with cosine distance strategy\n",
    "db_cosine = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = db_cosine.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created retriever with cosine similarity\")\n",
    "\n",
    "# Test the retriever\n",
    "test_query = \"excellent service\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"\\nüîç Test Query: '{test_query}'\")\n",
    "print(f\"üìä Retrieved {len(retrieved_docs)} documents\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs[:2], 1):\n",
    "    print(f\"Document {i}: {doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Different Search Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ Basic Similarity Search:\n",
      "Found 3 results\n",
      "Sample: comments: Staff went above and beyond to help us. Really appreciated their hospitality....\n",
      "\n",
      "2Ô∏è‚É£ MMR Search (for diverse results):\n",
      "Found 3 diverse results\n",
      "  1. comments: Loved the spa facilities. Very relaxing atmosphere throughout the hotel....\n",
      "  2. comments: Loved the spa facilities. Very relaxing atmosphere throughout the hotel....\n",
      "  3. comments: Loved the spa facilities. Very relaxing atmosphere throughout the hotel....\n",
      "\n",
      "3Ô∏è‚É£ Testing different query types:\n",
      "  Query: 'breakfast quality' - Best match (score: 0.514)\n",
      "    ‚Üí comments: Excellent service and beautiful rooms. The staff was very helpful and ...\n",
      "  Query: 'room cleanliness' - Best match (score: 0.566)\n",
      "    ‚Üí comments: The room was spotlessly clean and the bed was very comfortable. Great ...\n",
      "  Query: 'staff friendliness' - Best match (score: 0.430)\n",
      "    ‚Üí comments: Staff went above and beyond to help us. Really appreciated their hospi...\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic similarity search\n",
    "print(\"1Ô∏è‚É£ Basic Similarity Search:\")\n",
    "basic_results = db.similarity_search(\"excellent service\", k=3)\n",
    "print(f\"Found {len(basic_results)} results\")\n",
    "if basic_results:\n",
    "    print(f\"Sample: {basic_results[0].page_content[:150]}...\\n\")\n",
    "\n",
    "# 2. Maximum Marginal Relevance (MMR) search\n",
    "print(\"2Ô∏è‚É£ MMR Search (for diverse results):\")\n",
    "mmr_results = db.max_marginal_relevance_search(\n",
    "    \"hotel amenities\",\n",
    "    k=3,\n",
    "    fetch_k=10,\n",
    "    lambda_mult=0.5\n",
    ")\n",
    "print(f\"Found {len(mmr_results)} diverse results\")\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    print(f\"  {i}. {doc.page_content[:100]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 3. Test different query types\n",
    "print(\"3Ô∏è‚É£ Testing different query types:\")\n",
    "test_queries = [\n",
    "    \"breakfast quality\",\n",
    "    \"room cleanliness\", \n",
    "    \"staff friendliness\"\n",
    "]\n",
    "\n",
    "for test_query in test_queries:\n",
    "    results = db.similarity_search_with_score(test_query, k=1)\n",
    "    if results:\n",
    "        doc, score = results[0]\n",
    "        print(f\"  Query: '{test_query}' - Best match (score: {score:.3f})\")\n",
    "        print(f\"    ‚Üí {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "‚úÖ **Vector Embeddings**: Generated 768-dimensional embeddings using all-mpnet-base-v2  \n",
    "‚úÖ **pgvector Storage**: Stored embeddings in Aurora PostgreSQL with pgvector extension  \n",
    "‚úÖ **Similarity Search**: Retrieved semantically similar documents  \n",
    "‚úÖ **Score Calculation**: Computed cosine similarity scores  \n",
    "‚úÖ **LangChain Integration**: Created retrievers for use in chains  \n",
    "\n",
    "### Next Steps:\n",
    "- Scale to larger datasets\n",
    "- Integrate with LLMs for question-answering (RAG)\n",
    "- Optimize with IVFFlat or HNSW indexes\n",
    "- Experiment with different embedding models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
