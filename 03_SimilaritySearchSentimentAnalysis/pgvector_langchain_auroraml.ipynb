{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search with pgvector and Amazon Aurora PostgreSQL\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Use HuggingFace's sentence transformer model [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) to generate embeddings\n",
    "2. Store and query vector embeddings using pgvector in Aurora PostgreSQL  \n",
    "3. Implement semantic search using LangChain's vector store capabilities\n",
    "4. Calculate similarity scores between text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install required Python libraries for the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sentencepiece for tokenization (required by transformer models)\n",
    "# Suppress conda output\n",
    "!conda install -c conda-forge sentencepiece -y > /dev/null 2>&1\n",
    "print(\"‚úÖ Sentencepiece installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements1.txt\n",
    "# First set of dependencies\n",
    "langchain==0.2.16\n",
    "langchain-community==0.2.17\n",
    "langchain-postgres==0.0.15\n",
    "psycopg2-binary==2.9.10\n",
    "pgvector==0.2.5\n",
    "python-dotenv==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements2.txt\n",
    "# Second set of dependencies  \n",
    "sentence-transformers==2.2.2\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "torch\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages in two steps to avoid conflicts\n",
    "# Suppress pip output for cleaner display\n",
    "!pip install -r requirements1.txt -q 2>/dev/null\n",
    "!pip install -r requirements2.txt -q 2>/dev/null\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-source extension pgvector for PostgreSQL\n",
    "\n",
    "[pgvector](https://github.com/pgvector/pgvector) is an open-source extension for PostgreSQL that allows you to store and search vector embeddings for exact and approximate nearest neighbor search.\n",
    "\n",
    "Key features:\n",
    "- Store embeddings alongside regular data\n",
    "- Exact and approximate nearest neighbor search\n",
    "- L2, inner product, and cosine distance metrics\n",
    "- IVFFlat and HNSW indexes for fast search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and setup environment\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress all warnings before imports\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Disable CUDA to avoid GPU warnings\n",
    "\n",
    "# Suppress specific library warnings\n",
    "import sys\n",
    "if 'ipykernel' in sys.modules:\n",
    "    # Suppress tqdm warnings in notebook\n",
    "    import tqdm\n",
    "    tqdm.tqdm = tqdm.std.tqdm\n",
    "\n",
    "# Suppress transformers and torch warnings\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Disable torchvision beta warnings\n",
    "try:\n",
    "    import torchvision\n",
    "    torchvision.disable_beta_transforms_warning()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set logging level to ERROR only\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger('InstructorEmbedding').setLevel(logging.ERROR)\n",
    "logging.getLogger('sentence_transformers').setLevel(logging.ERROR)\n",
    "\n",
    "# Now import the rest of the libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_postgres import PGVector\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Try to use HuggingFaceInstructEmbeddings, fall back to regular HuggingFaceEmbeddings if not available\n",
    "try:\n",
    "    from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "    \n",
    "    # Suppress the INSTRUCTOR_Transformer loading message\n",
    "    import io\n",
    "    from contextlib import redirect_stdout, redirect_stderr\n",
    "    \n",
    "    with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "        embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "    print(\"‚úÖ Using HuggingFaceInstructEmbeddings\")\n",
    "    \n",
    "except (ImportError, Exception) as e:\n",
    "    # Fallback to regular HuggingFaceEmbeddings which works with the same model\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    print(\"‚úÖ Using HuggingFaceEmbeddings (fallback)\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üìä Using embedding model: all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "# Using environment variables from .env file\n",
    "\n",
    "DB_HOST = os.getenv('PGVECTOR_HOST')\n",
    "DB_PORT = os.getenv('PGVECTOR_PORT', '5432')\n",
    "DB_NAME = os.getenv('PGVECTOR_DATABASE')\n",
    "DB_USER = os.getenv('PGVECTOR_USER')\n",
    "DB_PASSWORD = os.getenv('PGVECTOR_PASSWORD')\n",
    "DB_DRIVER = os.getenv('PGVECTOR_DRIVER', 'psycopg2')\n",
    "\n",
    "# Build connection string\n",
    "CONNECTION_STRING = f\"postgresql+{DB_DRIVER}://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Collection name for vector store\n",
    "COLLECTION_NAME = \"hotel_reviews_langchain\"\n",
    "\n",
    "# Display configuration (masking password)\n",
    "display_connection = CONNECTION_STRING.replace(DB_PASSWORD, \"****\")\n",
    "print(f\"üìä Database Configuration:\")\n",
    "print(f\"   Host: {DB_HOST}\")\n",
    "print(f\"   Database: {DB_NAME}\")\n",
    "print(f\"   Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "Load hotel reviews data from CSV file. The file should have a 'comments' column with review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check for the actual data file\n",
    "data_file = './fictitious_hotel_reviews_trimmed_500.csv'\n",
    "if not os.path.exists(data_file):\n",
    "    # Try alternative path\n",
    "    data_file = './data/fictitious_hotel_reviews_trimmed_500.csv'\n",
    "    \n",
    "if not os.path.exists(data_file):\n",
    "    print(\"‚ö†Ô∏è Data file not found. Creating sample data...\")\n",
    "    # Create more diverse sample data if file doesn't exist\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    \n",
    "    sample_reviews = [\n",
    "        \"Excellent service and beautiful rooms. The staff was very helpful and the breakfast was amazing.\",\n",
    "        \"Great location near the beach. Pool area was fantastic! Very family friendly.\",\n",
    "        \"Amazing mountain views. Perfect for a peaceful getaway. Very quiet and relaxing.\",\n",
    "        \"Convenient location but rooms were a bit small. Good value for money though.\",\n",
    "        \"Beautiful lake views. Restaurant food was delicious. Will definitely come back.\",\n",
    "        \"The room was spotlessly clean and the bed was very comfortable. Great night's sleep.\",\n",
    "        \"Staff went above and beyond to help us. Really appreciated their hospitality.\",\n",
    "        \"Loved the spa facilities. Very relaxing atmosphere throughout the hotel.\",\n",
    "        \"Business center was well equipped. Perfect for work trips.\",\n",
    "        \"Kids loved the pool and game room. Great family vacation spot.\",\n",
    "        \"Room service was prompt and the food quality was excellent.\",\n",
    "        \"The concierge helped us plan our entire itinerary. Very knowledgeable.\",\n",
    "        \"Gym facilities were modern and well-maintained. Appreciated the 24-hour access.\",\n",
    "        \"The rooftop bar had amazing views of the city. Great cocktails too.\",\n",
    "        \"Breakfast buffet had lots of options including healthy choices.\",\n",
    "        \"Location was perfect - walking distance to all major attractions.\",\n",
    "        \"The hotel shuttle service to the airport was very convenient.\",\n",
    "        \"Loved the boutique feel of this hotel. Very unique decor.\",\n",
    "        \"Conference facilities were excellent for our business meeting.\",\n",
    "        \"The pet-friendly policy was great. Our dog was well taken care of.\"\n",
    "    ]\n",
    "    \n",
    "    # Create more varied data\n",
    "    import random\n",
    "    comments = []\n",
    "    for _ in range(100):\n",
    "        comments.append(random.choice(sample_reviews))\n",
    "    \n",
    "    sample_data = pd.DataFrame({'comments': comments})\n",
    "    data_file = './data/fictitious_hotel_reviews_trimmed_500.csv'\n",
    "    sample_data.to_csv(data_file, index=False)\n",
    "    print(f\"‚úÖ Created sample data file with {len(comments)} reviews\")\n",
    "\n",
    "# Load data using LangChain's CSVLoader\n",
    "# The CSVLoader will treat each row as a document\n",
    "loader = CSVLoader(\n",
    "    file_path=data_file,\n",
    "    encoding='utf-8',\n",
    "    csv_args={'delimiter': ','}\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(data)} documents from {data_file}\")\n",
    "print(f\"\\nFirst 3 reviews:\")\n",
    "for i, doc in enumerate(data[:3], 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Text into Chunks\n",
    "\n",
    "Split documents into smaller chunks for better retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text splitter\n",
    "# For hotel reviews, we might not need to split if reviews are already short\n",
    "# But we'll keep this for consistency with the original notebook\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"‚úÖ Split {len(data)} documents into {len(docs)} chunks\")\n",
    "print(f\"Average chunk size: {sum(len(d.page_content) for d in docs) / len(docs):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Collection\n",
    "\n",
    "Create pgvector collection and store document embeddings in Aurora PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "# Create PGVector instance and store documents\n",
    "# This will:\n",
    "# 1. Connect to Aurora PostgreSQL\n",
    "# 2. Create necessary tables if they don't exist\n",
    "# 3. Generate embeddings for all documents\n",
    "# 4. Store embeddings in the database\n",
    "\n",
    "print(\"üöÄ Creating vector store collection...\")\n",
    "print(\"‚è≥ This may take a minute...\")\n",
    "\n",
    "try:\n",
    "    db = PGVector.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        connection=CONNECTION_STRING,\n",
    "        pre_delete_collection=True  # Clean start - delete if exists\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Vector store created successfully!\")\n",
    "    print(f\"üìä Collection: {COLLECTION_NAME}\")\n",
    "    print(f\"üìù Documents indexed: {len(docs)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating vector store: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check database connection settings in .env file\")\n",
    "    print(\"2. Ensure pgvector extension is installed: CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    print(\"3. Verify database user has necessary permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search with Score\n",
    "\n",
    "Perform similarity search and retrieve documents with their similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search query\n",
    "query = \"What do some of the positive reviews say?\"\n",
    "\n",
    "# Perform similarity search with scores\n",
    "# Returns documents with their cosine similarity scores (0-1, higher is better)\n",
    "# Increase k to get more results, then filter duplicates\n",
    "docs_with_score = db.similarity_search_with_score(query, k=10)\n",
    "\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(f\"üìä Found {len(docs_with_score)} total matches\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display search results with scores\n",
    "# Show unique results with different content\n",
    "seen_content = set()\n",
    "unique_results = []\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    # Get first 100 chars of content for comparison\n",
    "    content_key = doc.page_content[:100]\n",
    "    if content_key not in seen_content:\n",
    "        seen_content.add(content_key)\n",
    "        unique_results.append((doc, score))\n",
    "\n",
    "print(f\"\\nüìä Showing {len(unique_results)} unique results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, (doc, score) in enumerate(unique_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"üìà Similarity Score: {score:.4f}\")\n",
    "    print(f\"üìÑ Content: {doc.page_content[:300]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cosine Similarity\n",
    "\n",
    "Use cosine distance strategy for similarity calculations and create a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import DistanceStrategy\n",
    "\n",
    "# Create a new vector store with cosine distance strategy\n",
    "db_cosine = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    distance_strategy=DistanceStrategy.COSINE  # Use cosine similarity\n",
    ")\n",
    "\n",
    "# Create a retriever for use in chains\n",
    "# This can be integrated with LangChain chains and agents\n",
    "retriever = db_cosine.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Return top 4 results\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created retriever with cosine similarity\")\n",
    "print(\"üìä Retriever will return top 4 most similar documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retriever\n",
    "query = 'What do some of the positive reviews say?'\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(f\"üìä Retrieved {len(retrieved_docs)} documents\\n\")\n",
    "\n",
    "# Display first two results\n",
    "for i, doc in enumerate(retrieved_docs[:2], 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Search Methods\n",
    "\n",
    "Explore different search methods available in LangChain with better handling of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic similarity search (without scores)\n",
    "print(\"1Ô∏è‚É£ Basic Similarity Search:\")\n",
    "basic_results = db.similarity_search(\"excellent service\", k=3)\n",
    "print(f\"Found {len(basic_results)} results\")\n",
    "\n",
    "# Show first unique result\n",
    "if basic_results:\n",
    "    print(f\"Sample: {basic_results[0].page_content[:150]}...\\n\")\n",
    "\n",
    "# 2. Maximum Marginal Relevance (MMR) search\n",
    "# Returns diverse results by balancing relevance and diversity\n",
    "print(\"2Ô∏è‚É£ MMR Search (for diverse results):\")\n",
    "mmr_results = db.max_marginal_relevance_search(\n",
    "    \"hotel amenities\",\n",
    "    k=3,\n",
    "    fetch_k=10,  # Fetch more candidates for diversity\n",
    "    lambda_mult=0.5  # Balance between relevance and diversity\n",
    ")\n",
    "print(f\"Found {len(mmr_results)} diverse results\")\n",
    "\n",
    "# Show unique MMR results\n",
    "seen = set()\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    content_key = doc.page_content[:50]\n",
    "    if content_key not in seen:\n",
    "        print(f\"  {i}. {doc.page_content[:100]}...\")\n",
    "        seen.add(content_key)\n",
    "\n",
    "print()\n",
    "\n",
    "# 3. Similarity search with different queries\n",
    "print(\"3Ô∏è‚É£ Testing different query types:\")\n",
    "test_queries = [\n",
    "    \"breakfast quality\",\n",
    "    \"room cleanliness\", \n",
    "    \"staff friendliness\"\n",
    "]\n",
    "\n",
    "for test_query in test_queries:\n",
    "    results = db.similarity_search_with_score(test_query, k=1)\n",
    "    if results:\n",
    "        doc, score = results[0]\n",
    "        print(f\"  Query: '{test_query}' - Best match (score: {score:.3f})\")\n",
    "        print(f\"    ‚Üí {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "‚úÖ **Vector Embeddings**: Generated 768-dimensional embeddings using all-mpnet-base-v2  \n",
    "‚úÖ **pgvector Storage**: Stored embeddings in Aurora PostgreSQL with pgvector extension  \n",
    "‚úÖ **Similarity Search**: Retrieved semantically similar documents  \n",
    "‚úÖ **Score Calculation**: Computed cosine similarity scores  \n",
    "‚úÖ **LangChain Integration**: Created retrievers for use in chains  \n",
    "\n",
    "### Next Steps:\n",
    "- Scale to larger datasets\n",
    "- Integrate with LLMs for question-answering (RAG)\n",
    "- Optimize with IVFFlat or HNSW indexes\n",
    "- Experiment with different embedding models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
