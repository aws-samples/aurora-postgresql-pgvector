{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search with pgvector and Amazon Aurora PostgreSQL\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Use HuggingFace's sentence transformer model [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) to generate embeddings\n",
    "2. Store and query vector embeddings using pgvector in Aurora PostgreSQL  \n",
    "3. Implement semantic search using LangChain's vector store capabilities\n",
    "4. Calculate similarity scores between text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Install required Python libraries for the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentencepiece installed\n"
     ]
    }
   ],
   "source": [
    "# Install sentencepiece for tokenization (required by transformer models)\n",
    "# Suppress conda output\n",
    "!conda install -c conda-forge sentencepiece -y > /dev/null 2>&1\n",
    "print(\"‚úÖ Sentencepiece installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements1.txt\n",
    "# First set of dependencies\n",
    "langchain==0.2.16\n",
    "langchain-community==0.2.17\n",
    "langchain-postgres==0.0.15\n",
    "psycopg2-binary==2.9.10\n",
    "pgvector==0.2.5\n",
    "python-dotenv==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements2.txt\n",
    "# Second set of dependencies  \n",
    "sentence-transformers==2.2.2\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "torch\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Install packages in two steps to avoid conflicts\n",
    "# Suppress pip output for cleaner display\n",
    "!pip install -r requirements1.txt -q 2>/dev/null\n",
    "!pip install -r requirements2.txt -q 2>/dev/null\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-source extension pgvector for PostgreSQL\n",
    "\n",
    "[pgvector](https://github.com/pgvector/pgvector) is an open-source extension for PostgreSQL that allows you to store and search vector embeddings for exact and approximate nearest neighbor search.\n",
    "\n",
    "Key features:\n",
    "- Store embeddings alongside regular data\n",
    "- Exact and approximate nearest neighbor search\n",
    "- L2, inner product, and cosine distance metrics\n",
    "- IVFFlat and HNSW indexes for fast search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using HuggingFaceInstructEmbeddings\n",
      "‚úÖ Environment setup complete!\n",
      "üìä Using embedding model: all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries and setup environment\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress all warnings before imports\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Disable CUDA to avoid GPU warnings\n",
    "\n",
    "# Suppress specific library warnings\n",
    "import sys\n",
    "if 'ipykernel' in sys.modules:\n",
    "    # Suppress tqdm warnings in notebook\n",
    "    import tqdm\n",
    "    tqdm.tqdm = tqdm.std.tqdm\n",
    "\n",
    "# Suppress transformers and torch warnings\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Disable torchvision beta warnings\n",
    "try:\n",
    "    import torchvision\n",
    "    torchvision.disable_beta_transforms_warning()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set logging level to ERROR only\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger('InstructorEmbedding').setLevel(logging.ERROR)\n",
    "logging.getLogger('sentence_transformers').setLevel(logging.ERROR)\n",
    "\n",
    "# Now import the rest of the libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_postgres import PGVector\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Try to use HuggingFaceInstructEmbeddings, fall back to regular HuggingFaceEmbeddings if not available\n",
    "try:\n",
    "    from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "    \n",
    "    # Suppress the INSTRUCTOR_Transformer loading message\n",
    "    import io\n",
    "    from contextlib import redirect_stdout, redirect_stderr\n",
    "    \n",
    "    with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):\n",
    "        embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "    print(\"‚úÖ Using HuggingFaceInstructEmbeddings\")\n",
    "    \n",
    "except (ImportError, Exception) as e:\n",
    "    # Fallback to regular HuggingFaceEmbeddings which works with the same model\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    print(\"‚úÖ Using HuggingFaceEmbeddings (fallback)\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üìä Using embedding model: all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Database Configuration:\n",
      "   Host: apgpg-pgvector.cluster-cng8i4a88jth.us-west-2.rds.amazonaws.com\n",
      "   Database: postgres\n",
      "   Collection: hotel_reviews_langchain\n"
     ]
    }
   ],
   "source": [
    "# Database connection configuration\n",
    "# Using environment variables from .env file\n",
    "\n",
    "DB_HOST = os.getenv('PGVECTOR_HOST')\n",
    "DB_PORT = os.getenv('PGVECTOR_PORT', '5432')\n",
    "DB_NAME = os.getenv('PGVECTOR_DATABASE')\n",
    "DB_USER = os.getenv('PGVECTOR_USER')\n",
    "DB_PASSWORD = os.getenv('PGVECTOR_PASSWORD')\n",
    "DB_DRIVER = os.getenv('PGVECTOR_DRIVER', 'psycopg2')\n",
    "\n",
    "# Build connection string\n",
    "CONNECTION_STRING = f\"postgresql+{DB_DRIVER}://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# Collection name for vector store\n",
    "COLLECTION_NAME = \"hotel_reviews_langchain\"\n",
    "\n",
    "# Display configuration (masking password)\n",
    "display_connection = CONNECTION_STRING.replace(DB_PASSWORD, \"****\")\n",
    "print(f\"üìä Database Configuration:\")\n",
    "print(f\"   Host: {DB_HOST}\")\n",
    "print(f\"   Database: {DB_NAME}\")\n",
    "print(f\"   Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "Load hotel reviews data from CSV file. The file should have a 'comments' column with review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 500 documents from ./data/fictitious_hotel_reviews_trimmed_500.csv\n",
      "\n",
      "First 3 reviews:\n",
      "\n",
      "1. Ôªøcomments: nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, che...\n",
      "\n",
      "2. Ôªøcomments: ok nothing special charge diamond member hilton decided chain shot 20th anniversary seattle, start booked suite paid extra website descript...\n",
      "\n",
      "3. Ôªøcomments: nice rooms not 4 star experience hotel monaco seattle good hotel n't 4 star level.positives large bathroom mediterranean suite comfortable ...\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check for the actual data file\n",
    "data_file = './fictitious_hotel_reviews_trimmed_500.csv'\n",
    "if not os.path.exists(data_file):\n",
    "    # Try alternative path\n",
    "    data_file = './data/fictitious_hotel_reviews_trimmed_500.csv'\n",
    "    \n",
    "if not os.path.exists(data_file):\n",
    "    print(\"‚ö†Ô∏è Data file not found. Creating sample data...\")\n",
    "    # Create more diverse sample data if file doesn't exist\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    \n",
    "    sample_reviews = [\n",
    "        \"Excellent service and beautiful rooms. The staff was very helpful and the breakfast was amazing.\",\n",
    "        \"Great location near the beach. Pool area was fantastic! Very family friendly.\",\n",
    "        \"Amazing mountain views. Perfect for a peaceful getaway. Very quiet and relaxing.\",\n",
    "        \"Convenient location but rooms were a bit small. Good value for money though.\",\n",
    "        \"Beautiful lake views. Restaurant food was delicious. Will definitely come back.\",\n",
    "        \"The room was spotlessly clean and the bed was very comfortable. Great night's sleep.\",\n",
    "        \"Staff went above and beyond to help us. Really appreciated their hospitality.\",\n",
    "        \"Loved the spa facilities. Very relaxing atmosphere throughout the hotel.\",\n",
    "        \"Business center was well equipped. Perfect for work trips.\",\n",
    "        \"Kids loved the pool and game room. Great family vacation spot.\",\n",
    "        \"Room service was prompt and the food quality was excellent.\",\n",
    "        \"The concierge helped us plan our entire itinerary. Very knowledgeable.\",\n",
    "        \"Gym facilities were modern and well-maintained. Appreciated the 24-hour access.\",\n",
    "        \"The rooftop bar had amazing views of the city. Great cocktails too.\",\n",
    "        \"Breakfast buffet had lots of options including healthy choices.\",\n",
    "        \"Location was perfect - walking distance to all major attractions.\",\n",
    "        \"The hotel shuttle service to the airport was very convenient.\",\n",
    "        \"Loved the boutique feel of this hotel. Very unique decor.\",\n",
    "        \"Conference facilities were excellent for our business meeting.\",\n",
    "        \"The pet-friendly policy was great. Our dog was well taken care of.\"\n",
    "    ]\n",
    "    \n",
    "    # Create more varied data\n",
    "    import random\n",
    "    comments = []\n",
    "    for _ in range(100):\n",
    "        comments.append(random.choice(sample_reviews))\n",
    "    \n",
    "    sample_data = pd.DataFrame({'comments': comments})\n",
    "    data_file = './data/fictitious_hotel_reviews_trimmed_500.csv'\n",
    "    sample_data.to_csv(data_file, index=False)\n",
    "    print(f\"‚úÖ Created sample data file with {len(comments)} reviews\")\n",
    "\n",
    "# Load data using LangChain's CSVLoader\n",
    "# The CSVLoader will treat each row as a document\n",
    "loader = CSVLoader(\n",
    "    file_path=data_file,\n",
    "    encoding='utf-8',\n",
    "    csv_args={'delimiter': ','}\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(data)} documents from {data_file}\")\n",
    "print(f\"\\nFirst 3 reviews:\")\n",
    "for i, doc in enumerate(data[:3], 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Text into Chunks\n",
    "\n",
    "Split documents into smaller chunks for better retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split 500 documents into 500 chunks\n",
      "Average chunk size: 528 characters\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "# For hotel reviews, we might not need to split if reviews are already short\n",
    "# But we'll keep this for consistency with the original notebook\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(f\"‚úÖ Split {len(data)} documents into {len(docs)} chunks\")\n",
    "print(f\"Average chunk size: {sum(len(d.page_content) for d in docs) / len(docs):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Collection\n",
    "\n",
    "Create pgvector collection and store document embeddings in Aurora PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating vector store collection...\n",
      "‚è≥ This may take a minute...\n",
      "‚úÖ Vector store created successfully!\n",
      "üìä Collection: hotel_reviews_langchain\n",
      "üìù Documents indexed: 500\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "# Create PGVector instance and store documents\n",
    "# This will:\n",
    "# 1. Connect to Aurora PostgreSQL\n",
    "# 2. Create necessary tables if they don't exist\n",
    "# 3. Generate embeddings for all documents\n",
    "# 4. Store embeddings in the database\n",
    "\n",
    "print(\"üöÄ Creating vector store collection...\")\n",
    "print(\"‚è≥ This may take a minute...\")\n",
    "\n",
    "try:\n",
    "    db = PGVector.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        connection=CONNECTION_STRING,\n",
    "        pre_delete_collection=True  # Clean start - delete if exists\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Vector store created successfully!\")\n",
    "    print(f\"üìä Collection: {COLLECTION_NAME}\")\n",
    "    print(f\"üìù Documents indexed: {len(docs)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating vector store: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check database connection settings in .env file\")\n",
    "    print(\"2. Ensure pgvector extension is installed: CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    print(\"3. Verify database user has necessary permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search with Score\n",
    "\n",
    "Perform similarity search and retrieve documents with their similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'What do some of the positive reviews say?'\n",
      "üìä Found 10 total matches\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define search query\n",
    "query = \"What do some of the positive reviews say?\"\n",
    "\n",
    "# Perform similarity search with scores\n",
    "# Returns documents with their cosine similarity scores (0-1, higher is better)\n",
    "# Increase k to get more results, then filter duplicates\n",
    "docs_with_score = db.similarity_search_with_score(query, k=10)\n",
    "\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(f\"üìä Found {len(docs_with_score)} total matches\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Showing 10 unique results:\n",
      "============================================================\n",
      "\n",
      "Result 1:\n",
      "üìà Similarity Score: 0.4225\n",
      "üìÑ Content: Ôªøcomments: decent expensive pros enjoyable stay, rooms bath clean beds crisp sheets, room appointed amenities like dvd audio players nice touch including furniture, minibar stocked items including toiletries, cons service good better price, great included items like free internet connectivity simple...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "üìà Similarity Score: 0.4233\n",
      "üìÑ Content: Ôªøcomments: better previous reviews suggest stayed downtown hilton recently nights conference, apprehensive stay given negative reviews travellers, positive experience stay did not use exercise facility business features, suggestion requested upper floor room view puget sound, quite pleasant, room cl...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 3:\n",
      "üìà Similarity Score: 0.4306\n",
      "üìÑ Content: Ôªøcomments: poor service good reviews andra gets makes wonder stayed place.my wife spent nights honeymoon seattle andra, wish days trip did n't anti-climax.having read previous bad review hotel contacted andra advance reserve room high floor away elevators soundproofing non-existent.what room did, lo...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 4:\n",
      "üìà Similarity Score: 0.4306\n",
      "üìÑ Content: Ôªøcomments: not bad price given central downtown location low price moore good value two-star, price not fancy adequate clean roomy, elevator hallways small dingy rooms fine, beds good tv sufficient channel selection telephone bathroom normal size adequate motel-type towels/face cloths soap no shampo...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 5:\n",
      "üìà Similarity Score: 0.4449\n",
      "üìÑ Content: Ôªøcomments: affordable great location price boyfriend stayed 2 nights weekend getaway seattle 80us night needed, staff friendly room clean, great budget hotel felt safe, parking hotel 8/day, barely spent time room choose hotel convenient location things wanted, walking distance mcdonalds starbucks sh...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 6:\n",
      "üìà Similarity Score: 0.4458\n",
      "üìÑ Content: Ôªøcomments: nice hotel expensive parking got good deal stay hotel anniversary, arrived late evening took advice previous reviews did valet parking, check quick easy, little disappointed non-existent view room room clean nice size, bed comfortable woke stiff neck high pillows, not soundproof like hear...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 7:\n",
      "üìà Similarity Score: 0.4558\n",
      "üìÑ Content: Ôªøcomments: nice hotel friendly staff no problem checking parking reviews suggested, hotels seattle charge parking went expecting, parking garage clean lit equipped elevator lobby hotel.the staff friendly met request smile, morning beverage option nice touch kids enjoyed, rooms clean maintained,...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 8:\n",
      "üìà Similarity Score: 0.4573\n",
      "üìÑ Content: Ôªøcomments: stay reviewers stated room quite small design fresh hip, location city not better, staff friendly helpful, negative available space w/in room taken mini-fridge expensive snacks nice little storage instead stayed 3 nights, beds comfy rooms/bathrooms clean, shower separate room quite small ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 9:\n",
      "üìà Similarity Score: 0.4579\n",
      "üìÑ Content: Ôªøcomments: great location great service great location, staff accomodating, free upgrade check-in dest staff, asked ok wheelchair accessible room said yes not, did not care bathroom water pressure low 36th floor room, asked change morning switched 34th floor room hassles, stayed 3 nights enjoyed sta...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Result 10:\n",
      "üìà Similarity Score: 0.4587\n",
      "üìÑ Content: Ôªøcomments: reading reviews felt good idea expect, rooms definitely small, crazy small, bathroom barely fits sink toilet room shower rooms diff, configurations bathrooms split, basically room bed long desk flat screen tv propped, talks great customer service opposite experience, got late driving 5 ho...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display search results with scores\n",
    "# Show unique results with different content\n",
    "seen_content = set()\n",
    "unique_results = []\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    # Get first 100 chars of content for comparison\n",
    "    content_key = doc.page_content[:100]\n",
    "    if content_key not in seen_content:\n",
    "        seen_content.add(content_key)\n",
    "        unique_results.append((doc, score))\n",
    "\n",
    "print(f\"\\nüìä Showing {len(unique_results)} unique results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, (doc, score) in enumerate(unique_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"üìà Similarity Score: {score:.4f}\")\n",
    "    print(f\"üìÑ Content: {doc.page_content[:300]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Cosine Similarity\n",
    "\n",
    "Use cosine distance strategy for similarity calculations and create a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created retriever with cosine similarity\n",
      "üìä Retriever will return top 4 most similar documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_postgres.vectorstores import DistanceStrategy\n",
    "\n",
    "# Create a new vector store with cosine distance strategy\n",
    "db_cosine = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    distance_strategy=DistanceStrategy.COSINE  # Use cosine similarity\n",
    ")\n",
    "\n",
    "# Create a retriever for use in chains\n",
    "# This can be integrated with LangChain chains and agents\n",
    "retriever = db_cosine.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Return top 4 results\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created retriever with cosine similarity\")\n",
    "print(\"üìä Retriever will return top 4 most similar documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'What do some of the positive reviews say?'\n",
      "üìä Retrieved 4 documents\n",
      "\n",
      "Document 1:\n",
      "Ôªøcomments: decent expensive pros enjoyable stay, rooms bath clean beds crisp sheets, room appointed amenities like dvd audio players nice touch including furniture, minibar stocked items including toi...\n",
      "\n",
      "Document 2:\n",
      "Ôªøcomments: better previous reviews suggest stayed downtown hilton recently nights conference, apprehensive stay given negative reviews travellers, positive experience stay did not use exercise facilit...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the retriever\n",
    "query = 'What do some of the positive reviews say?'\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(f\"üìä Retrieved {len(retrieved_docs)} documents\\n\")\n",
    "\n",
    "# Display first two results\n",
    "for i, doc in enumerate(retrieved_docs[:2], 1):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"{doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Search Methods\n",
    "\n",
    "Explore different search methods available in LangChain with better handling of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ Basic Similarity Search:\n",
      "Found 3 results\n",
      "Sample: Ôªøcomments: excellent staff, housekeeping quality hotel chocked staff make feel home, experienced exceptional service desk staff concierge door men mai...\n",
      "\n",
      "2Ô∏è‚É£ MMR Search (for diverse results):\n",
      "Found 3 diverse results\n",
      "  1. Ôªøcomments: basic hotel basic needs hotel perfect young travellers just need place sleep no needs far...\n",
      "  2. Ôªøcomments: comfortable pleasant stay stayed hotel nights attend conference, room comfortable hotel l...\n",
      "  3. Ôªøcomments: fair room quality unfriendly staff make sure reservation traveling group team, motel not ...\n",
      "\n",
      "3Ô∏è‚É£ Testing different query types:\n",
      "  Query: 'breakfast quality' - Best match (score: 0.545)\n",
      "    ‚Üí Ôªøcomments: decent expensive pros enjoyable stay, rooms bath clean beds crisp she...\n",
      "  Query: 'room cleanliness' - Best match (score: 0.439)\n",
      "    ‚Üí Ôªøcomments: n't bother dump, door room warped room hallway stunk big time, beddin...\n",
      "  Query: 'staff friendliness' - Best match (score: 0.455)\n",
      "    ‚Üí Ôªøcomments: left hand does n't know right doing desk clerks not helpful getting t...\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic similarity search (without scores)\n",
    "print(\"1Ô∏è‚É£ Basic Similarity Search:\")\n",
    "basic_results = db.similarity_search(\"excellent service\", k=3)\n",
    "print(f\"Found {len(basic_results)} results\")\n",
    "\n",
    "# Show first unique result\n",
    "if basic_results:\n",
    "    print(f\"Sample: {basic_results[0].page_content[:150]}...\\n\")\n",
    "\n",
    "# 2. Maximum Marginal Relevance (MMR) search\n",
    "# Returns diverse results by balancing relevance and diversity\n",
    "print(\"2Ô∏è‚É£ MMR Search (for diverse results):\")\n",
    "mmr_results = db.max_marginal_relevance_search(\n",
    "    \"hotel amenities\",\n",
    "    k=3,\n",
    "    fetch_k=10,  # Fetch more candidates for diversity\n",
    "    lambda_mult=0.5  # Balance between relevance and diversity\n",
    ")\n",
    "print(f\"Found {len(mmr_results)} diverse results\")\n",
    "\n",
    "# Show unique MMR results\n",
    "seen = set()\n",
    "for i, doc in enumerate(mmr_results, 1):\n",
    "    content_key = doc.page_content[:50]\n",
    "    if content_key not in seen:\n",
    "        print(f\"  {i}. {doc.page_content[:100]}...\")\n",
    "        seen.add(content_key)\n",
    "\n",
    "print()\n",
    "\n",
    "# 3. Similarity search with different queries\n",
    "print(\"3Ô∏è‚É£ Testing different query types:\")\n",
    "test_queries = [\n",
    "    \"breakfast quality\",\n",
    "    \"room cleanliness\", \n",
    "    \"staff friendliness\"\n",
    "]\n",
    "\n",
    "for test_query in test_queries:\n",
    "    results = db.similarity_search_with_score(test_query, k=1)\n",
    "    if results:\n",
    "        doc, score = results[0]\n",
    "        print(f\"  Query: '{test_query}' - Best match (score: {score:.3f})\")\n",
    "        print(f\"    ‚Üí {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "‚úÖ **Vector Embeddings**: Generated 768-dimensional embeddings using all-mpnet-base-v2  \n",
    "‚úÖ **pgvector Storage**: Stored embeddings in Aurora PostgreSQL with pgvector extension  \n",
    "‚úÖ **Similarity Search**: Retrieved semantically similar documents  \n",
    "‚úÖ **Score Calculation**: Computed cosine similarity scores  \n",
    "‚úÖ **LangChain Integration**: Created retrievers for use in chains  \n",
    "\n",
    "### Next Steps:\n",
    "- Scale to larger datasets\n",
    "- Integrate with LLMs for question-answering (RAG)\n",
    "- Optimize with IVFFlat or HNSW indexes\n",
    "- Experiment with different embedding models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
