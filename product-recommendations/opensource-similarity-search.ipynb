{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01cb58e",
   "metadata": {},
   "source": [
    "# Building AI-powered search in PostgreSQL using Amazon SageMaker and pgvector\n",
    "_**Using a pretrained LLM and PostgreSQL extension `pgvector` for similarity search on product catalog**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Language Translation](#Language-Translation)\n",
    "1. [SageMaker Model Hosting](#SageMaker-Model-Hosting)\n",
    "1. [Load data into PostgreSQL](#PostgreSQL-vector-extension)\n",
    "1. [Evaluate Search Results](#Evaluate-PostgreSQL-vector-Search-Results)\n",
    "\n",
    "## Background\n",
    "\n",
    "In this notebook, we'll build the core components of a textually similar Products. Often people don't know what exactly they are looking for and in that case they just type an item description and hope it will retrieve similar items.\n",
    "\n",
    "One of the core components of searching textually similar items is a fixed length sentence/word embedding i.e. a  “feature vector” that corresponds to that text. The reference word/sentence embedding typically are generated offline and must be stored so they can be efficiently searched. In this use case we are using a pretrained SentenceTransformer model `all-MiniLM-L6-v2` from [HuggingFace Transformers](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n",
    "\n",
    "To enable efficient searches for textually similar items, we'll use Amazon SageMaker to generate fixed length sentence embeddings i.e “feature vectors” and use the Nearest Neighbor search in Amazon Aurora for PostgreSQL using the extension `pgvector`. The PostgreSQL `pgvector` extension lets you store and search for points in vector space and find the \"nearest neighbors\" for those points. Use cases include recommendations (for example, an \"other songs you might like\" feature in a music application), image recognition, and fraud detection.\n",
    "\n",
    "Here are the steps we'll follow to build textually similar items: After some initial setup, we'll host the pretrained language model in SageMaker. Then generate feature vectors for Fashion products from *__feidegger__*, a *__zalandoresearch__* dataset. Those feature vectors will be stored in Amazon Aurora for PostgreSQL vector datatype. Next, we'll explore some sample text queries, and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7045906",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required python libraries for the workshop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb79cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r opensource_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283ad62",
   "metadata": {},
   "source": [
    "### Downloading Zalando Research data\n",
    "\n",
    "The dataset itself consists of 8,732 high-resolution images, each depicting a dress from the available on the Zalando shop against a white-background. Each of the images has five textual annotations orinally in German and are translated to english, each of which has been generated by a separate user. \n",
    "\n",
    "**Downloading Zalando Research data**: Data originally from here: https://github.com/zalandoresearch/feidegger \n",
    "\n",
    " **Citation:** <br>\n",
    " https://github.com/zalandoresearch/feidegger <br>\n",
    " *@inproceedings{lefakis2018feidegger,* <br>\n",
    " *title={FEIDEGGER: A Multi-modal Corpus of Fashion Images and Descriptions in German},* <br>\n",
    " *author={Lefakis, Leonidas and Akbik, Alan and Vollgraf, Roland},* <br>\n",
    " *booktitle = {{LREC} 2018, 11th Language Resources and Evaluation Conference},* <br>\n",
    " *year      = {2018}* <br>\n",
    " *}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "filename = 'data/FEIDEGGER_release_1.2.json'\n",
    "\n",
    "with open(filename) as json_file:\n",
    "    results = json.load(json_file)\n",
    "\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9cd0d4",
   "metadata": {},
   "source": [
    "## SageMaker Model Hosting\n",
    "\n",
    "In this section will deploy the pretrained `all-MiniLM-L6-v2` Hugging Face SentenceTransformer model into SageMaker and generate 384 dimensional vector embeddings for our product catalog descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c409edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20daf917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying the opensource module. The process takes around 5 min to complete.\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. <https://huggingface.co/models>\n",
    "hub = {\n",
    "  'HF_MODEL_ID': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "  'HF_TASK': 'feature-extraction'\n",
    "}\n",
    "\n",
    "# Deploy Hugging Face Model \n",
    "predictor = HuggingFaceModel(\n",
    "               env=hub, # configuration for loading model from Hub\n",
    "               role=role, # iam role with permissions to create an Endpoint\n",
    "               transformers_version='4.26',\n",
    "               pytorch_version='1.13',\n",
    "               py_version='py39',\n",
    "            ).deploy(\n",
    "               initial_instance_count=1,\n",
    "               instance_type=\"ml.m5.xlarge\",\n",
    "               endpoint_name=\"apg-vector\"\n",
    "            )\n",
    "print(f\"Hugging Face Model has been deployed successfully to SageMaker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76357427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    #First element of model_output contains all token embeddings\n",
    "    return [sublist[0] for sublist in model_output][0]\n",
    "\n",
    "data = {\n",
    "  \"inputs\": ' '.join(results[0].get('descriptions'))\n",
    "}\n",
    "\n",
    "res = cls_pooling( predictor.predict(data=data) )\n",
    "print (len(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dbc0f-61de-4cf2-a6ae-86051ab4b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please re-run the cell if it fails.\n",
    "# Perform a job using realtime inference to generate embeddings ~30 min.\n",
    "\n",
    "def generate_embeddings(data):\n",
    "    r = {}\n",
    "    r['url'] = data['url']\n",
    "    r['descriptions'] = data['descriptions']\n",
    "    r['split'] = data['split']\n",
    "    inp = {'inputs' : ' '.join( data['descriptions'] ) }\n",
    "    vector = cls_pooling( predictor.predict(inp) )\n",
    "    r['descriptions_embeddings'] = vector\n",
    "    return r\n",
    "    \n",
    "workers = 1 * cpu_count()\n",
    "\n",
    "chunksize = 32\n",
    "\n",
    "#Generate Embeddings\n",
    "data = process_map(generate_embeddings, results, max_workers=workers, chunksize=chunksize)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abbfac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[0].get('descriptions_embeddings'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e107d",
   "metadata": {},
   "source": [
    "## Open-source extension pgvector in PostgreSQL\n",
    "\n",
    "pgvector is an open-source extension for PostgreSQL that allows you to store and search vector embeddings for exact and approximate nearest neighbors. It is designed to work seamlessly with other PostgreSQL features, including indexing and querying.\n",
    "\n",
    "One of the key benefits of using pgvector is that it allows you to perform similarity searches on large datasets quickly and efficiently. This is particularly useful in industries like e-commerce, where businesses need to be able to quickly search through large product catalogs to find the items that best match a customer's preferences. It supports exact and approximate nearest neighbor search, L2 distance, inner product, and cosine distance.\n",
    "\n",
    "To further optimize your searches, you can also use pgvector's indexing features. By creating indexes on your vector data, you can speed up your searches and reduce the amount of time it takes to find the nearest neighbors to a given vector.\n",
    "\n",
    "In this step we'll get all the translated product descriptions of *__zalandoresearch__* dataset and store those embeddings into PostgreSQL vector type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468de6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import boto3 \n",
    "import json \n",
    "\n",
    "client = boto3.client('secretsmanager')\n",
    "\n",
    "response = client.get_secret_value(\n",
    "    SecretId='apgpg-pgvector-secret'\n",
    ")\n",
    "database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "\n",
    "dbconn = psycopg2.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10)\n",
    "dbconn.set_session(autocommit=True)\n",
    "\n",
    "cur = dbconn.cursor()\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "register_vector(dbconn)\n",
    "cur.execute(\"DROP TABLE IF EXISTS products;\")\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS products(\n",
    "               id bigserial primary key, \n",
    "               description text, \n",
    "               url text, \n",
    "               split int, \n",
    "               descriptions_embeddings vector(384));\"\"\")\n",
    "\n",
    "for x in data:\n",
    "    cur.execute(\"\"\"INSERT INTO products\n",
    "                      (description, url, split, descriptions_embeddings) \n",
    "                  VALUES(%s, %s, %s, %s);\"\"\", \n",
    "                  (' '.join(x.get('descriptions', [])), x.get('url'), x.get('split'), x.get('descriptions_embeddings') ))\n",
    "\n",
    "cur.execute(\"\"\"CREATE INDEX ON products \n",
    "               USING ivfflat (descriptions_embeddings vector_l2_ops) WITH (lists = 100);\"\"\")\n",
    "cur.execute(\"VACUUM ANALYZE products;\")\n",
    "\n",
    "cur.close()\n",
    "dbconn.close()\n",
    "print (\"Vector embeddings has been successfully loaded into PostgreSQL\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a93851",
   "metadata": {},
   "source": [
    "## Evaluate PostgreSQL vector Search Results\n",
    "\n",
    "In this step we will use SageMaker realtime inference to generate embeddings for the query and use the embeddings to search the PostgreSQL to retrive the nearest neighbours and retrive the relevent product images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea144f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "def similarity_search(search_text):\n",
    "    \n",
    "    data = {\"inputs\": search_text}\n",
    "    res1 = cls_pooling( predictor.predict(data\n",
    "                                          =data) )\n",
    "    client = boto3.client('secretsmanager')\n",
    "\n",
    "    response = client.get_secret_value(\n",
    "        SecretId='apgpg-pgvector-secret'\n",
    "    )\n",
    "    database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "    dbhost = database_secrets['host']\n",
    "    dbport = database_secrets['port']\n",
    "    dbuser = database_secrets['username']\n",
    "    dbpass = database_secrets['password']\n",
    "\n",
    "    dbconn = psycopg2.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, connect_timeout=10)\n",
    "    dbconn.set_session(autocommit=True)\n",
    "    cur = dbconn.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"SELECT id, url, description, descriptions_embeddings \n",
    "                FROM products \n",
    "                ORDER BY descriptions_embeddings <-> %s limit 2;\"\"\", \n",
    "                (np.array(res1),))\n",
    "\n",
    "    r = cur.fetchall()\n",
    "    urls = []\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "    for x in r:\n",
    "        #print (x)\n",
    "        url = x[1].split('?')[0]\n",
    "        urldata = requests.get(url).content\n",
    "        print (\"Product Item Id: \" + str(x[0]))\n",
    "        a = io.imread(url)\n",
    "        plt.imshow(a)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    cur.close()\n",
    "    dbconn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8812e7",
   "metadata": {},
   "source": [
    "Using the above function `similarity_search` , lets do some search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_search(\"red sleeveless summer wear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b32947",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_search(\"floral design pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d4001-56d4-420e-a9da-43969d7c0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
